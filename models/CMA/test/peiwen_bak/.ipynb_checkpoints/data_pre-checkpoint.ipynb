{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start!\n",
      "finish!\n"
     ]
    }
   ],
   "source": [
    "from scipy.signal import hilbert\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as func\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "from parameters import *\n",
    "\n",
    "\n",
    "def add_delay(data):\n",
    "    sound = data.iloc[:, 0]\n",
    "    sound_not_target = data.iloc[:, 1]\n",
    "    EEG = data.iloc[:, 2:]\n",
    "\n",
    "    if delay >= 0:\n",
    "        sound = sound.iloc[:sound.shape[0] - delay]\n",
    "        sound_not_target = sound_not_target.iloc[:sound_not_target.shape[0] - delay]\n",
    "        EEG = EEG.iloc[delay:, :]\n",
    "    else:\n",
    "        sound = sound.iloc[-delay:]\n",
    "        sound_not_target = sound_not_target.iloc[-delay:]\n",
    "        EEG = EEG.iloc[:EEG.shape[0] + delay, :]\n",
    "\n",
    "    sound = sound.reset_index(drop=True)\n",
    "    sound_not_target = sound_not_target.reset_index(drop=True)\n",
    "    EEG = EEG.reset_index(drop=True)\n",
    "\n",
    "    data_pf = pd.concat([sound, sound_not_target, EEG],\n",
    "                        axis=1, ignore_index=True)\n",
    "    return data_pf\n",
    "\n",
    "\n",
    "def read(name, fb_index = None):\n",
    "    first = True\n",
    "    data_pf = pd.DataFrame()\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    for l in range(len(ConType)):\n",
    "        sex = pd.read_csv(\"./csv/\" + name + ConType[l] + \".csv\")\n",
    "        for k in range(trail_number):\n",
    "            # 读取数据\n",
    "            filename = data_document_path + \"/\" + ConType[l] + \"/\" + name + \"Tra\" + str(k + 1)\n",
    "            filename = filename + \"_\" + str(fb_index) if bands_number != 1 else filename\n",
    "            filename = filename + \".csv\"\n",
    "            train_data = pd.read_csv(filename, header=None)\n",
    "\n",
    "            EEG_data = train_data.iloc[:, 2:]\n",
    "            Sound_data = train_data.iloc[:, 0]\n",
    "            Sound_data_not_target = train_data.iloc[:, 1]\n",
    "\n",
    "            # 调整左右位置，添加辅助信息\n",
    "            if isDS and sex.iloc[k, isFM] == 2:\n",
    "                temp = Sound_data\n",
    "                Sound_data = Sound_data_not_target\n",
    "                Sound_data_not_target = temp\n",
    "\n",
    "            # 合并\n",
    "            EEG_data = pd.DataFrame(EEG_data)\n",
    "            Sound_data = pd.DataFrame(Sound_data)\n",
    "            Sound_data_not_target = pd.DataFrame(Sound_data_not_target)\n",
    "            data_pf = pd.concat(\n",
    "                [Sound_data, Sound_data_not_target, EEG_data], axis=1, ignore_index=True)\n",
    "\n",
    "            # 加入时延\n",
    "            data_pf = add_delay(data_pf)\n",
    "\n",
    "            if first:\n",
    "                data = data_pf\n",
    "                first = False\n",
    "            else:\n",
    "                data = pd.concat([data, data_pf], axis=0, ignore_index=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def timeSplit(data, name):\n",
    "    def update_CNN_DS_S(data_pf, data_direction, temp_data, temp_direction):\n",
    "        # 如果是CNN：D+S或CNN：FM+S模型\n",
    "        if isDS:\n",
    "            data_pf = pd.concat([data_pf, temp_data],\n",
    "                                axis=0, ignore_index=True)\n",
    "            data_direction.append(temp_direction)\n",
    "        # 否则是CNN：S模型\n",
    "        else:\n",
    "            temp = np.array(temp_data)\n",
    "            data_pf = pd.concat([data_pf, pd.DataFrame(\n",
    "                temp.copy())], axis=0, ignore_index=True)\n",
    "            temp[:, [0, channel_number - 1]] = temp[:, [channel_number - 1, 0]]\n",
    "            data_pf = pd.concat([data_pf, pd.DataFrame(\n",
    "                temp.copy())], axis=0, ignore_index=True)\n",
    "            data_direction.append(1)\n",
    "            data_direction.append(2)\n",
    "        return data_pf, data_direction\n",
    "\n",
    "    # 参数初始化\n",
    "    global cell_number\n",
    "    global test_percent\n",
    "    cell_number = cell_number - abs(delay)\n",
    "    window_lap = window_length * (1 - overlap)\n",
    "    overlap_distance = math.floor(1 / (1 - overlap)) - 1\n",
    "    selection_trails = 0\n",
    "    if isBeyoudTrail:\n",
    "        selection_trails = random.sample(\n",
    "            range(trail_number), math.ceil(trail_number * test_percent))\n",
    "\n",
    "    # 找不到其他空矩阵创建方法，先用着\n",
    "    data_pf = pd.DataFrame(data.iloc[:window_length, :])\n",
    "    test_pf = pd.DataFrame(data.iloc[:window_length, :])\n",
    "    data_direction = []\n",
    "    test_direction = []\n",
    "\n",
    "    # 对于每个ConType进行划分\n",
    "    for l in range(len(ConType)):\n",
    "        sex = pd.read_csv(\"./csv/\" + name + ConType[l] + \".csv\")\n",
    "\n",
    "        # 对于ConType里的每个trail进行划分\n",
    "        for k in range(trail_number):\n",
    "            # 每个trail里的窗口个数\n",
    "            window_number = math.floor(\n",
    "                (cell_number - window_length) / window_lap) + 1\n",
    "            # 随机抽取的测试窗口长度\n",
    "            if isBeyoudTrail:\n",
    "                test_percent = 1 if k in selection_trails else 0\n",
    "            test_percent = 0 if isALLTrain else test_percent\n",
    "            test_window_length = math.floor(\n",
    "                (cell_number * test_percent - window_length) / window_lap)\n",
    "            test_window_length = test_window_length if test_percent == 0 else max(\n",
    "                0, test_window_length)\n",
    "            test_window_length = test_window_length + 1\n",
    "            # 随机抽取的测试窗口左右边界\n",
    "            test_window_left = random.randint(\n",
    "                0, window_number - test_window_length)\n",
    "            test_window_right = test_window_left + test_window_length - 1\n",
    "\n",
    "            # 对于ConType里的trail里的每个窗口进行划分\n",
    "            for i in range(window_number):\n",
    "                left = math.floor(k * cell_number + i * window_lap)\n",
    "                right = math.floor(left + window_length)\n",
    "                # 如果不是要抽取的测试窗口，即为训练集里的窗口\n",
    "                if test_window_left > test_window_right or test_window_left - i > overlap_distance or i - test_window_right > overlap_distance:\n",
    "                    temp_data = data.iloc[left:right, :]\n",
    "                    temp_direction = sex.iloc[k, isFM]\n",
    "                    data_pf, data_direction = update_CNN_DS_S(\n",
    "                        data_pf, data_direction, temp_data, temp_direction)\n",
    "                elif i >= test_window_left and i <= test_window_right:\n",
    "                    temp_data = data.iloc[left:right, :]\n",
    "                    temp_direction = sex.iloc[k, isFM]\n",
    "                    test_pf, test_direction = update_CNN_DS_S(\n",
    "                        test_pf, test_direction, temp_data, temp_direction)\n",
    "\n",
    "    # 去除初始化的数据\n",
    "    data_pf = data_pf.iloc[window_length:, :]\n",
    "    test_pf = test_pf.iloc[window_length:, :]\n",
    "\n",
    "    # 重新组织结构\n",
    "    data_pf = np.array(data_pf).reshape(-1, window_length, channel_number)\n",
    "    test_pf = np.array(test_pf).reshape(-1, window_length, channel_number)\n",
    "\n",
    "    data = []\n",
    "    for i in range(data_pf.shape[0]):\n",
    "        d = dict()\n",
    "        d[\"data\"] = data_pf[i]\n",
    "        d[\"direction\"] = data_direction[i]\n",
    "        d[\"index\"] = i\n",
    "        data.append(d)\n",
    "\n",
    "    test = []\n",
    "    for i in range(test_pf.shape[0]):\n",
    "        d = dict()\n",
    "        d[\"data\"] = test_pf[i]\n",
    "        d[\"direction\"] = test_direction[i]\n",
    "        d[\"index\"] = i\n",
    "        test.append(d)\n",
    "\n",
    "    return data, test\n",
    "\n",
    "\n",
    "def change(train_data):\n",
    "    train_sound = train_data.iloc[:, 0]\n",
    "    train_sound_not_target = train_data.iloc[:, 1]\n",
    "    train_EEG = train_data.iloc[:, 2:]\n",
    "    data_pf = pd.concat(\n",
    "        [train_sound, train_EEG, train_sound_not_target], axis=1, ignore_index=True)\n",
    "    return data_pf\n",
    "\n",
    "\n",
    "def main(name=\"S2\"):\n",
    "    print(\"start!\")\n",
    "    \n",
    "    # 提取不同频带数据并汇总得到训练集和测试集\n",
    "    max_band = bands_number\n",
    "    train = []\n",
    "    test = []\n",
    "    for i in range(max_band):\n",
    "        # 读取数据\n",
    "        data_pf = read(name, i+1)\n",
    "        # 调整数据结构\n",
    "        data_pf = change(data_pf)\n",
    "        # 划分时间窗口并生成训练集和测试集\n",
    "        newtrain, newtest = timeSplit(data_pf, name)\n",
    "        train += newtrain\n",
    "        test += newtest\n",
    "\n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "\n",
    "    # 合并和保存结果\n",
    "    data_pf = [train, test]\n",
    "    data_pf = np.array(data_pf)\n",
    "    np.save(\"./data_new/CNN1_\" + name, data_pf)\n",
    "\n",
    "    print(\"finish!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if (len(sys.argv) == 2):\n",
    "        main(sys.argv[1])\n",
    "    else:\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start!\n",
      "[ 85 231 101 535 214]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lipei\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:49: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ce02854db26c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m         \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-ce02854db26c>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;31m# print(data[0].size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m         \u001b[0mtrainEpoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-ce02854db26c>\u001b[0m in \u001b[0;36mtrainEpoch\u001b[1;34m(data, test_data)\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mloss_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargetIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mloss2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-ce02854db26c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train, epoch, targetIndex)\u001b[0m\n\u001b[0;32m    103\u001b[0m                         y_label=\"eeg\")\n\u001b[0;32m    104\u001b[0m                 heatmap(eaA[-1], epoch, itemIndex, \"eaA_last\", title=\"cmA q=eeg kv=audio\" + title_str, x_label=\"audio\",\n\u001b[1;32m--> 105\u001b[1;33m                         y_label=\"eeg\")\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                 heatmap(aeB[0], epoch, itemIndex, \"aeB_first\", title=\"cmB q=audio kv=eeg\" + title_str, x_label=\"eeg\",\n",
      "\u001b[1;32m<ipython-input-3-ce02854db26c>\u001b[0m in \u001b[0;36mheatmap\u001b[1;34m(_, epoch, number, matrix_num, title, x_label, y_label)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m#     print(a.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mheatmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquare\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\matrix.py\u001b[0m in \u001b[0;36mheatmap\u001b[1;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msquare\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_aspect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"equal\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m     \u001b[0mplotter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbar_ax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\matrix.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, ax, cax, kws)\u001b[0m\n\u001b[0;32m    310\u001b[0m             \u001b[0myticks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myticklabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myticklabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myticks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myticks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m         \u001b[0mxtl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xticklabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxticklabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[0mytl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_yticklabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myticklabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"vertical\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1069\u001b[0m             sorted(kwargs.items(), reverse=True,\n\u001b[0;32m   1070\u001b[0m                    key=lambda x: (self._prop_order.get(x[0], 0), x[0])))\n\u001b[1;32m-> 1071\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfindobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_self\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, props)\u001b[0m\n\u001b[0;32m    972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meventson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 974\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_update_property\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meventson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 974\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_update_property\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36m_update_property\u001b[1;34m(self, k, v)\u001b[0m\n\u001b[0;32m    969\u001b[0m                     raise AttributeError('{!r} object has no property {!r}'\n\u001b[0;32m    970\u001b[0m                                          .format(type(self).__name__, k))\n\u001b[1;32m--> 971\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meventson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36mset_xticks\u001b[1;34m(self, ticks, minor)\u001b[0m\n\u001b[0;32m   3347\u001b[0m             \u001b[0mDefault\u001b[0m \u001b[1;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3348\u001b[0m         \"\"\"\n\u001b[1;32m-> 3349\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mminor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3350\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3351\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mset_ticks\u001b[1;34m(self, ticks, minor)\u001b[0m\n\u001b[0;32m   1761\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1762\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_major_locator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmticker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFixedLocator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1763\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_major_ticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1765\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_tick_boxes_siblings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mget_major_ticks\u001b[1;34m(self, numticks)\u001b[0m\n\u001b[0;32m   1409\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mnumticks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1410\u001b[0m             \u001b[1;31m# Update the new tick label properties from the old.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1411\u001b[1;33m             \u001b[0mtick\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmajor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1412\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtick\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1413\u001b[0m             \u001b[0mtick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgridline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gridOnMajor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36m_get_tick\u001b[1;34m(self, major)\u001b[0m\n\u001b[0;32m   1931\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1932\u001b[0m             \u001b[0mtick_kw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_minor_tick_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1933\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mXTick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmajor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmajor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtick_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1935\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, axes, loc, label, size, width, color, tickdir, pad, labelsize, labelcolor, zorder, gridOn, tick1On, tick2On, label1On, label2On, major, labelrotation, grid_color, grid_linestyle, grid_linewidth, grid_alpha, **kw)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick1line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tick1line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick2line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tick2line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgridline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_gridline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_text1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36m_get_tick2line\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    476\u001b[0m                           \u001b[0mmarkersize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m                           \u001b[0mmarkeredgewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_width\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m                           zorder=self._zorder)\n\u001b[0m\u001b[0;32m    479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m         \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_xaxis_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhich\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tick2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, xdata, ydata, linewidth, linestyle, color, marker, markersize, markeredgewidth, markeredgecolor, markerfacecolor, markerfacecoloralt, fillstyle, antialiased, dash_capstyle, solid_capstyle, dash_joinstyle, solid_joinstyle, pickradius, drawstyle, markevery, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_color\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_marker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMarkerStyle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfillstyle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_markevery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\markers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, marker, fillstyle)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_marker_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_fillstyle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfillstyle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_marker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_recache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\markers.py\u001b[0m in \u001b[0;36mset_marker\u001b[1;34m(self, marker)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_marker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\markers.py\u001b[0m in \u001b[0;36m_recache\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_capstyle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'butt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_marker_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\markers.py\u001b[0m in \u001b[0;36m_set_tickup\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_tickup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAffine2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    764\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_snap_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\transforms.py\u001b[0m in \u001b[0;36mscale\u001b[1;34m(self, sx, sy)\u001b[0m\n\u001b[0;32m   1989\u001b[0m         scale_mtx = np.array(\n\u001b[0;32m   1990\u001b[0m             [[sx, 0.0, 0.0], [0.0, sy, 0.0], [0.0, 0.0, 1.0]], float)\n\u001b[1;32m-> 1991\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscale_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1992\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1993\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.signal import hilbert\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as func\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "from parameters import *\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def vali_split(train):\n",
    "    window_number = train.shape[0]\n",
    "    # 随机抽取的验证窗口长度\n",
    "    vali_window_length = math.floor(window_number * vali_percent)\n",
    "    # 随机抽取的验证窗口\n",
    "    vali_window_left = random.randint(0, window_number - vali_window_length)\n",
    "    vali_window_right = vali_window_left + vali_window_length - 1\n",
    "    # 重复距离\n",
    "    overlap_distance = math.floor(1 / (1 - overlap)) - 1\n",
    "\n",
    "    train_window = []\n",
    "    vali_window = []\n",
    "\n",
    "    for i in range(window_number):\n",
    "        # 如果不是要抽取的验证窗口\n",
    "        if vali_window_left - i > overlap_distance or i - vali_window_right > overlap_distance:\n",
    "            train_window.append(train[i])\n",
    "        elif i >= vali_window_left and i <= vali_window_right:\n",
    "            vali_window.append(train[i])\n",
    "\n",
    "    return np.array(train_window), np.array(vali_window)\n",
    "\n",
    "\n",
    "def heatmap(_, epoch, number, matrix_num, title='crossmodal attention matrix', x_label='No.', y_label='No.'):\n",
    "    #     print(type(_[0]))\n",
    "    #     print(_[0].cpu().detach().numpy().shape)\n",
    "    a = _[0].cpu().detach().numpy()\n",
    "    #     print(a.shape)\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    heatmap = sns.heatmap(pd.DataFrame(np.round(a, 2)), xticklabels=True, yticklabels=True, square=True, center=0)\n",
    "    ax.set_title(title, fontsize=18)\n",
    "    ax.set_ylabel(y_label, fontsize=18)\n",
    "    ax.set_xlabel(x_label, fontsize=18)  # 横变成y轴，跟矩阵原始的布局情况是一样的\n",
    "\n",
    "    fig.savefig(\"./picture/\" + names[0] + \"/\" + str(epoch) + \"_\" + str(number) + \"_\" + str(matrix_num) + \".png\")\n",
    "\n",
    "\n",
    "def isLeftPredicted(result):\n",
    "    result = result.cpu().detach().numpy()\n",
    "    result = np.expand_dims(result, axis=0)\n",
    "    result = torch.from_numpy(result).to(device)\n",
    "    lossL = loss_func(result, torch.tensor([0]).to(device)).cpu().detach().numpy()\n",
    "    lossR = loss_func(result, torch.tensor([1]).to(device)).cpu().detach().numpy()\n",
    "    return lossL < lossR\n",
    "\n",
    "\n",
    "def train(train, epoch, targetIndex):\n",
    "    losses = 0\n",
    "\n",
    "    for turn in range(math.floor(train.shape[0] / batch_size)):\n",
    "        optimzer.zero_grad()\n",
    "        temp = train[turn][\"data\"].T\n",
    "        batchData = np.ndarray((0, 1, temp.shape[0], temp.shape[1]))\n",
    "        allTarget = []\n",
    "        for k in range(batch_size):\n",
    "            input = train[turn * batch_size + k][\"data\"].T\n",
    "            input = np.expand_dims(input, axis=0)\n",
    "            input = np.expand_dims(input, axis=0)\n",
    "            batchData = np.concatenate((batchData, input), axis=0)\n",
    "            target = [train[turn * batch_size + k][\"direction\"] - 1]\n",
    "            allTarget = np.concatenate((allTarget, target), axis=0)\n",
    "        x = torch.tensor(batchData, dtype=torch.float32)\n",
    "        x = x.to(device)\n",
    "        out, aeA, audio, ae_fcA, eaA, aeB, eaB = myNet(x)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            itemIndex = train[turn * batch_size + i][\"index\"]\n",
    "            isLeft = isLeftPredicted(out[i])\n",
    "            isAAttend = allTarget[i] == 0\n",
    "            if itemIndex in targetIndex:\n",
    "                ma1 = pd.DataFrame(ae_fcA[-1].cpu().detach().numpy())\n",
    "                ma1.to_csv(\n",
    "                    \"./picture/\" + names[0] + \"/\" + str(epoch) + \"_\" + str(itemIndex) + \"_qkv.csv\")\n",
    "\n",
    "                attend_str = \" attend=A\" if isAAttend else \" attend=A\"\n",
    "                predict_str = \" predict=A\" if isLeft else \" predict=B\"\n",
    "                title_str = attend_str + predict_str\n",
    "                heatmap(aeA[0], epoch, itemIndex, \"aeA_first\", title=\"cmA q=audio kv=eeg\" + title_str, x_label=\"eeg\",\n",
    "                        y_label=\"audio\")\n",
    "                heatmap(aeA[-1], epoch, itemIndex, \"aeA_last\", title=\"cmA q=audio kv=eeg\" + title_str, x_label=\"eeg\",\n",
    "                        y_label=\"audio\")\n",
    "                heatmap(eaA[0], epoch, itemIndex, \"eaA_first\", title=\"cmA q=eeg kv=audio\" + title_str, x_label=\"audio\",\n",
    "                        y_label=\"eeg\")\n",
    "                heatmap(eaA[-1], epoch, itemIndex, \"eaA_last\", title=\"cmA q=eeg kv=audio\" + title_str, x_label=\"audio\",\n",
    "                        y_label=\"eeg\")\n",
    "\n",
    "                heatmap(aeB[0], epoch, itemIndex, \"aeB_first\", title=\"cmB q=audio kv=eeg\" + title_str, x_label=\"eeg\",\n",
    "                        y_label=\"audio\")\n",
    "                heatmap(aeB[-1], epoch, itemIndex, \"aeB_last\", title=\"cmB q=audio kv=eeg\" + title_str, x_label=\"eeg\",\n",
    "                        y_label=\"audio\")\n",
    "                heatmap(eaB[0], epoch, itemIndex, \"eaB_first\", title=\"cmB q=eeg kv=audio\" + title_str, x_label=\"audio\",\n",
    "                        y_label=\"eeg\")\n",
    "                heatmap(eaB[-1], epoch, itemIndex, \"eaB_last\", title=\"cmB q=eeg kv=audio\" + title_str, x_label=\"audio\",\n",
    "                        y_label=\"eeg\")\n",
    "\n",
    "                heatmap([ae_fcA[-1]], epoch, itemIndex, \"qkv\", title=\"qkv weight\")\n",
    "                heatmap(audio[0], epoch, itemIndex, \"wavA_before\", title=\"wavA_before\")\n",
    "                heatmap(audio[1], epoch, itemIndex, \"wavA_after\", title=\"wavA_after\")\n",
    "\n",
    "        loss = loss_func(out, torch.tensor(allTarget, dtype=torch.long).to(device))\n",
    "        losses = losses + loss.cpu().detach().numpy()\n",
    "        loss.backward()\n",
    "        optimzer.step()\n",
    "    # scheduler.step()\n",
    "    scheduler.step(metrics=0.1)\n",
    "\n",
    "    return losses / (math.floor(train.shape[0] / batch_size))\n",
    "\n",
    "\n",
    "def test(cv):\n",
    "    losses = 0\n",
    "\n",
    "    for turn in range(math.floor(cv.shape[0] / batch_size)):\n",
    "        optimzer.zero_grad()\n",
    "        temp = cv[turn][\"data\"].T\n",
    "        batchData = np.ndarray((0, 1, temp.shape[0], temp.shape[1]))\n",
    "        allTarget = []\n",
    "        for k in range(batch_size):\n",
    "            input = cv[turn * batch_size + k][\"data\"].T\n",
    "            input = np.expand_dims(input, axis=0)\n",
    "            input = np.expand_dims(input, axis=0)\n",
    "            batchData = np.concatenate((batchData, input), axis=0)\n",
    "            target = [cv[turn * batch_size + k][\"direction\"] - 1]\n",
    "            allTarget = np.concatenate((allTarget, target), axis=0)\n",
    "        x = torch.tensor(batchData, dtype=torch.float32)\n",
    "        x = x.to(device)\n",
    "        out, aeA, audio, ae_fcA, eaA, aeB, eaB = myNet(x)\n",
    "        loss = loss_func(out, torch.tensor(allTarget, dtype=torch.long).to(device))\n",
    "        losses = losses + loss.cpu().detach().numpy()\n",
    "\n",
    "    return losses / (math.floor(cv.shape[0] / batch_size))\n",
    "\n",
    "\n",
    "def trainEpoch(data, test_data):\n",
    "    min_loss = 100\n",
    "    early_stop_number = 0\n",
    "\n",
    "    targetIndex = np.random.choice(a=math.floor(data[0].shape[0] * 0.9), size=5, replace=False)\n",
    "    print(targetIndex)\n",
    "\n",
    "    for epoch in range(max_epoch):\n",
    "\n",
    "        # 打乱非测试数据集并划分训练集和验证集\n",
    "        dataset = data[0].copy()\n",
    "        train_data, cv_data = vali_split(dataset)\n",
    "        np.random.shuffle(train_data)\n",
    "\n",
    "        loss_train = train(train_data, epoch, targetIndex)\n",
    "        loss = test(cv_data)\n",
    "        loss2 = test(test_data)\n",
    "\n",
    "        print(str(epoch) + \" \" + str(loss_train) + \" \" + str(loss) + \" \" + str(loss2), end=\"\")\n",
    "\n",
    "        if loss > min_loss:\n",
    "            early_stop_number = early_stop_number + 1\n",
    "        else:\n",
    "            early_stop_number = 0\n",
    "            min_loss = loss\n",
    "\n",
    "        print(\" early_stop_number: \", end=\"\")\n",
    "        print(early_stop_number, end=\"\")\n",
    "        print()\n",
    "\n",
    "        if isEarlyStop and epoch > min_epoch and early_stop_number >= 10:\n",
    "            break\n",
    "\n",
    "\n",
    "def testEpoch(test_data):\n",
    "    for num in range(10):\n",
    "        t_num = 0\n",
    "        f_num = 0\n",
    "        for turn in range(math.floor(test_data.shape[0] / batch_size)):\n",
    "            optimzer.zero_grad()\n",
    "            temp = test_data[turn][\"data\"].T\n",
    "            batchData = np.ndarray((0, 1, temp.shape[0], temp.shape[1]))\n",
    "            allTarget = []\n",
    "            for k in range(batch_size):\n",
    "                input = test_data[turn * batch_size + k][\"data\"].T\n",
    "                input = np.expand_dims(input, axis=0)\n",
    "                input = np.expand_dims(input, axis=0)\n",
    "                batchData = np.concatenate((batchData, input), axis=0)\n",
    "                target = [test_data[turn * batch_size + k][\"direction\"] - 1]\n",
    "                allTarget = np.concatenate((allTarget, target), axis=0)\n",
    "            x = torch.tensor(batchData, dtype=torch.float32)\n",
    "            x = x.to(device)\n",
    "            out, aeA, audio, ae_fcA, eaA, aeB, eaB = myNet(x)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                ifLeft = isLeftPredicted(out[i])\n",
    "                if ifLeft == (allTarget[i] == 0):\n",
    "                    t_num = t_num + 1\n",
    "                else:\n",
    "                    f_num = f_num + 1\n",
    "\n",
    "        print(str(t_num) + \" \" + str(f_num))\n",
    "    print(str(t_num / (t_num + f_num)))\n",
    "\n",
    "\n",
    "def main(name=\"S2\"):\n",
    "    # 参数init\n",
    "    name_number = int(name[1:])\n",
    "\n",
    "    # 先读取测试数据\n",
    "    data = np.load(\"./data_new/CNN1_\" + name + \".npy\", allow_pickle=True)\n",
    "    test_data = data[0] if isALLTrain and need_pretrain and not need_train else data[1]\n",
    "\n",
    "    # 读取数据并预训练\n",
    "    if need_pretrain:\n",
    "        print(\"pretrain start!\")\n",
    "        basic_name = \"S\" + str(name_number % (people_number - 1) + 1)\n",
    "        b = np.load(\"./data_new/CNN1_\" + basic_name + \".npy\", allow_pickle=True)\n",
    "        for k in range(people_number):\n",
    "            filelable = \"S\" + str(k + 1)\n",
    "            if (not isALLTrain or filelable != name) and filelable != basic_name:\n",
    "                # 读取数据\n",
    "                a = np.load(\"./data_new/CNN1_\" + filelable + \".npy\", allow_pickle=True)\n",
    "                b[0] = np.hstack((a[0], b[0]))\n",
    "                b[1] = np.hstack((a[1], b[1]))\n",
    "        data = b\n",
    "        trainEpoch(data, test_data)\n",
    "        print()\n",
    "\n",
    "    # 读取数据并训练\n",
    "    if need_train:\n",
    "        # 降低学习率\n",
    "        if need_pretrain:\n",
    "            for p in optimzer.param_groups:\n",
    "                p['lr'] *= 0.1\n",
    "\n",
    "        print(\"train start!\")\n",
    "        data = np.load(\"./data_new/CNN1_\" + name + \".npy\", allow_pickle=True)\n",
    "\n",
    "        # # 随机选取N个数 临时起作用\n",
    "        # np.random.shuffle(data[0])\n",
    "        # print(data[0].size)\n",
    "        # length = math.floor(data[0].size / (700 / window_length))\n",
    "        # data[0] = data[0][:length]\n",
    "        # print(data[0].size)\n",
    "\n",
    "        trainEpoch(data, test_data)\n",
    "        print()\n",
    "\n",
    "    # 测试\n",
    "    print(\"test start!\")\n",
    "    testEpoch(test_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if (len(sys.argv) > 1 and sys.argv[1].startswith(\"S\")):\n",
    "        main(sys.argv[1])\n",
    "    else:\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from scipy.signal import hilbert\n",
      "import numpy as np\n",
      "import scipy.io as scio\n",
      "import pandas as pd\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import matplotlib.pyplot as plt\n",
      "import torch.nn.functional as func\n",
      "from torch.autograd import Function\n",
      "from torch.autograd import Variable\n",
      "import math\n",
      "import random\n",
      "import sys\n",
      "import os\n",
      "from modules.transformer import TransformerEncoder\n",
      "\n",
      "# 使用的参数\n",
      "# 输入数据选择\n",
      "# label 为该次训练的标识\n",
      "# ConType 为选用数据的声学环境，如果ConType = [\"No\", \"Low\", \"High\"]，则将三种声学数据混合在一起后进行训练\n",
      "# names 为这次训练用到的被试数据\n",
      "label = \"cm_textCNN_4cm_heatmap\"\n",
      "ConType = [\"No\"]\n",
      "# names = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\", \"S6\", \"S7\", \"S8\", \"S9\", \"S10\", \"S11\", \"S12\", \"S13\", \"S14\", \"S15\", \"S16\", \"S17\",\n",
      "#          \"S18\"]\n",
      "# names = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\", \"S6\", \"S7\", \"S8\", \"S9\", \"S10\", \"S11\", \"S12\", \"S13\", \"S14\", \"S15\", \"S16\", \"S17\", \"S18\"]\n",
      "names = [\"S2\"]\n",
      "\n",
      "# 所用的数据目录路径\n",
      "# data_document_path = \"../dataset\"\n",
      "data_document_path = \"../dataset_16\"\n",
      "# data_document_path = \"../dataset_csp\"\n",
      "# data_document_path = \"../dataset_fbcsp\"\n",
      "# data_document_path = \"../data_all_split\"\n",
      "# data_document_path = \"/document/data/eeg/dataset\"\n",
      "# data_document_path = \"/document/data/eeg/dataset_csp\"\n",
      "# data_document_path = \"/document/data/eeg/dataset_fbcsp\"\n",
      "# data_document_path = \"/document/data/eeg/dataset_fbcsp10\"\n",
      "# data_document_path = \"/document/data/eeg/data_all_split\"\n",
      "\n",
      "# 常用模型参数，分别是 重复率、窗长、时延、最大迭代次数、分批训练参数、是否early stop\n",
      "# 其中窗长和时延，因为采样率为70hz，所以70为1秒\n",
      "overlap = 0.5\n",
      "window_length = 140\n",
      "delay = 0\n",
      "batch_size = 1\n",
      "max_epoch = 60\n",
      "min_epoch = 0\n",
      "isEarlyStop = False\n",
      "\n",
      "# 非常用参数，分别是 被试数量、通道数量、trail数量、trail内数据点数量、测试集比例、验证集比例\n",
      "# 一般不需要调整\n",
      "people_number = 18\n",
      "eeg_channel = 16\n",
      "audio_channel = 1\n",
      "channel_number = eeg_channel + audio_channel * 2\n",
      "trail_number = 20\n",
      "cell_number = 3500\n",
      "test_percent = 0.1\n",
      "vali_percent = 0.1\n",
      "\n",
      "conv_eeg_channel = 16\n",
      "conv_audio_channel = 16\n",
      "conv_time_size = 9\n",
      "conv_output_channel = 10\n",
      "fc_number = 30\n",
      "\n",
      "conv_eeg_audio_number = 4\n",
      "output_fc_number = conv_eeg_audio_number * conv_output_channel\n",
      "\n",
      "# 模型选择\n",
      "# True为CNN：D+S或CNN：FM+S模型，False为CNN：S模型\n",
      "isDS = True\n",
      "# isFM为0是男女信息，为1是方向信息\n",
      "isFM = 0\n",
      "# 频带数量\n",
      "bands_number = 1\n",
      "useFB = False\n",
      "\n",
      "# 数据划分选择\n",
      "# 测试集划分是否跨trail\n",
      "isBeyoudTrail = False\n",
      "# 是否使用100%的数据作为训练集，isBeyoudTrail=False、isALLTrain=True、need_pretrain = True、need_train = False说明跨被试\n",
      "isALLTrain = False\n",
      "\n",
      "# 预训练选择\n",
      "# 只有train就是单独训练、只有pretrain是跨被试、两者都有是预训练\n",
      "# 跨被试还需要上方的 isALLTrain 为 True\n",
      "need_pretrain = False\n",
      "need_train = True\n",
      "\n",
      "\n",
      "# 整体模型\n",
      "\n",
      "\n",
      "class CNN(nn.Module):\n",
      "    def __init__(self, embed_dim=30,\n",
      "                 num_heads=5,\n",
      "                 layers=5,\n",
      "                 attn_dropout=0.1,\n",
      "                 relu_dropout=0.1,\n",
      "                 res_dropout=0.1,\n",
      "                 embed_dropout=0.25,\n",
      "                 attn_mask=True):\n",
      "        super(CNN, self).__init__()\n",
      "\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        self.layers = layers\n",
      "        self.attn_dropout = attn_dropout\n",
      "        self.relu_dropout = relu_dropout\n",
      "        self.res_dropout = res_dropout\n",
      "        self.embed_dropout = embed_dropout\n",
      "        self.attn_mask = attn_mask\n",
      "\n",
      "        self.conv1 = nn.ModuleList([nn.Sequential(\n",
      "            nn.Conv1d(conv_audio_channel, conv_output_channel, 8),\n",
      "            nn.ReLU(),\n",
      "            nn.AdaptiveMaxPool1d(1),\n",
      "        ) for i in range(conv_eeg_audio_number)])\n",
      "\n",
      "        self.conv2 = nn.ModuleList([nn.Sequential(\n",
      "            nn.Conv1d(conv_audio_channel, conv_output_channel, 9),\n",
      "            nn.ReLU(),\n",
      "            nn.AdaptiveMaxPool1d(1),\n",
      "        ) for i in range(conv_eeg_audio_number)])\n",
      "\n",
      "        self.conv3 = nn.ModuleList([nn.Sequential(\n",
      "            nn.Conv1d(conv_audio_channel, conv_output_channel, 10),\n",
      "            nn.ReLU(),\n",
      "            nn.AdaptiveMaxPool1d(1),\n",
      "        ) for i in range(conv_eeg_audio_number)])\n",
      "\n",
      "        self.fc = nn.ModuleList([nn.Sequential(\n",
      "            nn.Linear(fc_number, fc_number),\n",
      "            nn.ReLU(),\n",
      "            nn.Dropout(p=0.1),\n",
      "            nn.Linear(fc_number, conv_output_channel)\n",
      "        ) for i in range(conv_eeg_audio_number)])\n",
      "\n",
      "        self.output_fc = nn.Sequential(\n",
      "            nn.Linear(output_fc_number, output_fc_number), nn.Sigmoid(),\n",
      "            nn.Linear(output_fc_number, 2), nn.Sigmoid()\n",
      "        )\n",
      "\n",
      "        self.proj_images = nn.Conv1d(eeg_channel, conv_eeg_channel, 1, padding=0, bias=False)\n",
      "        self.proj_images2 = nn.Conv1d(eeg_channel, conv_eeg_channel, 1, padding=0, bias=False)\n",
      "        self.proj_audio = nn.Conv1d(audio_channel, conv_audio_channel, 1, bias=False)\n",
      "        self.proj_audio2 = nn.Conv1d(audio_channel, conv_audio_channel, 1, bias=False)\n",
      "\n",
      "        self.trans_a2e = self.get_network(self_type='a2e')\n",
      "        self.trans_a2e2 = self.get_network(self_type='a2e')\n",
      "        self.trans_e2a = self.get_network(self_type='e2a')\n",
      "        self.trans_e2a2 = self.get_network(self_type='e2a')\n",
      "\n",
      "    def get_network(self, self_type='a2e', layers=-1):\n",
      "        if self_type in ['a2e']:\n",
      "            embed_dim = 16\n",
      "            num_heads = 1\n",
      "            isAE = True\n",
      "        elif self_type in ['e2a']:\n",
      "            embed_dim = 16\n",
      "            num_heads = 1\n",
      "            isAE = False\n",
      "        else:\n",
      "            raise ValueError(\"Unknown network type\")\n",
      "\n",
      "        return TransformerEncoder(embed_dim=embed_dim,\n",
      "                                  num_heads=num_heads,\n",
      "                                  layers=max(self.layers, layers),\n",
      "                                  attn_dropout=self.attn_dropout,\n",
      "                                  relu_dropout=self.relu_dropout,\n",
      "                                  res_dropout=self.res_dropout,\n",
      "                                  embed_dropout=self.embed_dropout,\n",
      "                                  attn_mask=self.attn_mask,\n",
      "                                  isAE=isAE)\n",
      "\n",
      "    def cross_modal(self, x_audio, x_eeg, is_images=False, conv1D=False):\n",
      "        '''\n",
      "        audio and eeg should have dimension [batch_size, seq_len, n_features]\n",
      "        if x_eeg is eegImages, the dimension should be [batch_size, seq_len, channel, height, width], default the images channel=1\n",
      "        x_audio and x_eeg are both numpy array.\n",
      "        '''\n",
      "\n",
      "        x_audio = x_audio.transpose(1, 2)\n",
      "        x_eeg = x_eeg.transpose(1, 2)\n",
      "\n",
      "        #     proj_x_audio = self.proj_audio(x_audio)\n",
      "        #     proj_x_eeg = self.proj_images(x_eeg)\n",
      "\n",
      "        # proj_x_audio = proj_x_audio.permute(2, 0, 1)\n",
      "        # proj_x_eeg = proj_x_eeg.permute(2, 0, 1)\n",
      "        x_audio = x_audio.permute(2, 0, 1)\n",
      "        x_eeg = x_eeg.permute(2, 0, 1)\n",
      "\n",
      "        audio_trans_eeg, ae, ae_fc = self.trans_a2e(x_audio, x_eeg, x_eeg, True)\n",
      "        audio_trans_eeg = audio_trans_eeg.permute(1, 0, 2)\n",
      "        eeg_trans_audio, ea, ea_fc = self.trans_e2a(x_eeg, x_audio, x_audio, True)\n",
      "        eeg_trans_audio = eeg_trans_audio.permute(1, 0, 2)\n",
      "\n",
      "        return eeg_trans_audio, audio_trans_eeg, ae, ae_fc, ea, ea_fc\n",
      "\n",
      "    def cross_modal2(self, x_audio, x_eeg, is_images=False, conv1D=False):\n",
      "        '''\n",
      "        audio and eeg should have dimension [batch_size, seq_len, n_features]\n",
      "        if x_eeg is eegImages, the dimension should be [batch_size, seq_len, channel, height, width], default the images channel=1\n",
      "        x_audio and x_eeg are both numpy array.\n",
      "        '''\n",
      "\n",
      "        x_audio = x_audio.transpose(1, 2)\n",
      "        x_eeg = x_eeg.transpose(1, 2)\n",
      "\n",
      "        #     proj_x_audio = self.proj_audio2(x_audio)\n",
      "        #     proj_x_eeg = self.proj_images2(x_eeg)\n",
      "\n",
      "        # proj_x_audio = proj_x_audio.permute(2, 0, 1)\n",
      "        # proj_x_eeg = proj_x_eeg.permute(2, 0, 1)\n",
      "        x_audio = x_audio.permute(2, 0, 1)\n",
      "        x_eeg = x_eeg.permute(2, 0, 1)\n",
      "\n",
      "        audio_trans_eeg, ae, ae_fc = self.trans_a2e(x_audio, x_eeg, x_eeg, True)\n",
      "        audio_trans_eeg = audio_trans_eeg.permute(1, 0, 2)\n",
      "        eeg_trans_audio, ea, ea_fc = self.trans_e2a2(x_eeg, x_audio, x_audio, True)\n",
      "        eeg_trans_audio = eeg_trans_audio.permute(1, 0, 2)\n",
      "\n",
      "        return eeg_trans_audio, audio_trans_eeg, ae, ae_fc, ea, ea_fc\n",
      "\n",
      "    def forward(self, x):\n",
      "\n",
      "        wavA = x[0, 0, 0:1, :]\n",
      "        wavA = torch.t(wavA).unsqueeze(0)\n",
      "        eeg = x[0, 0, 1:17, :]\n",
      "        eeg = torch.t(eeg).unsqueeze(0)\n",
      "        wavB = x[0, 0, 17:18, :]\n",
      "        wavB = torch.t(wavB).unsqueeze(0)\n",
      "\n",
      "        # print(x.shape)\n",
      "        # print(wavA.shape)\n",
      "        # print(eeg.shape)\n",
      "        # print(wavB.shape)\n",
      "\n",
      "        wavA = torch.zeros(wavA.shape[0], wavA.shape[1], conv_audio_channel).to(device) + wavA\n",
      "        wavB = torch.zeros(wavA.shape[0], wavB.shape[1], conv_audio_channel).to(device) + wavB\n",
      "\n",
      "        wavA_before = wavA.clone()\n",
      "\n",
      "        eegA, wavA, aeA, ae_fcA, eaA, ea_fcA = self.cross_modal(wavA, eeg, is_images=False, conv1D=True)\n",
      "        eegB, wavB, aeB, ae_fcB, eaB, ea_fcB = self.cross_modal2(wavB, eeg, is_images=False, conv1D=True)\n",
      "\n",
      "        wavA_after = wavA.clone()\n",
      "\n",
      "        # print(\"wavAB\")\n",
      "        # print(wavA.shape)\n",
      "        # print(wavB.shape)\n",
      "\n",
      "        wavA = wavA.transpose(1, 2)\n",
      "        eegA = eegA.transpose(1, 2)\n",
      "        eegB = eegB.transpose(1, 2)\n",
      "        wavB = wavB.transpose(1, 2)\n",
      "\n",
      "        # textCNN\n",
      "        data = [wavA, eegA, eegB, wavB]\n",
      "        for i in range(conv_eeg_audio_number):\n",
      "            # print(wavA.shape)\n",
      "            temp1 = self.conv1[i](data[i]).view(-1, conv_output_channel)\n",
      "            # print(wavA1.shape)\n",
      "            temp2 = self.conv2[i](data[i]).view(-1, conv_output_channel)\n",
      "            # print(wavA2.shape)\n",
      "            temp3 = self.conv3[i](data[i]).view(-1, conv_output_channel)\n",
      "            # print(wavA3.shape)\n",
      "            temp = torch.cat([temp1, temp2, temp3], dim=1)\n",
      "            data[i] = self.fc[i](temp)\n",
      "\n",
      "        x = torch.cat([data[i] for i in range(conv_eeg_audio_number)], dim=1)\n",
      "        output = self.output_fc(x)\n",
      "        return output, aeA, [wavA_before, wavA_after], ae_fcA, eaA, aeB, eaB\n",
      "\n",
      "\n",
      "# 模型权重初始化\n",
      "def weights_init_uniform(m):\n",
      "    classname = m.__class__.__name__\n",
      "    if classname.find('Linear') != -1:\n",
      "        m.weight.data.uniform_(-1.0, 1.0)\n",
      "        m.bias.data.fill_(0)\n",
      "\n",
      "\n",
      "# 模型参数和初始化\n",
      "myNet = CNN()\n",
      "myNet.apply(weights_init_uniform)\n",
      "clip = 0.8\n",
      "optimzer = torch.optim.Adam(myNet.parameters(), lr=0.1)\n",
      "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
      "    optimzer, mode='min', factor=0.5, patience=5, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=5,\n",
      "    min_lr=0.1, eps=0.001)\n",
      "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimzer, T_max = 10, eta_min=0, last_epoch=-1)\n",
      "loss_func = nn.CrossEntropyLoss()\n",
      "\n",
      "# 启用gpu\n",
      "gpu_random = random.randint(5, 7)\n",
      "device = torch.device('cuda:' + str(0))\n",
      "# device = torch.device('cpu')\n",
      "myNet = myNet.to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_func = loss_func.to(device)\n",
      "from scipy.signal import hilbert\n",
      "import numpy as np\n",
      "import scipy.io as scio\n",
      "import pandas as pd\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import matplotlib.pyplot as plt\n",
      "import torch.nn.functional as func\n",
      "from torch.autograd import Function\n",
      "from torch.autograd import Variable\n",
      "import math\n",
      "import random\n",
      "import sys\n",
      "import os\n",
      "from modules.transformer import TransformerEncoder\n",
      "\n",
      "# 使用的参数\n",
      "# 输入数据选择\n",
      "# label 为该次训练的标识\n",
      "# ConType 为选用数据的声学环境，如果ConType = [\"No\", \"Low\", \"High\"]，则将三种声学数据混合在一起后进行训练\n",
      "# names 为这次训练用到的被试数据\n",
      "label = \"cm_textCNN_4cm_heatmap\"\n",
      "ConType = [\"No\"]\n",
      "# names = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\", \"S6\", \"S7\", \"S8\", \"S9\", \"S10\", \"S11\", \"S12\", \"S13\", \"S14\", \"S15\", \"S16\", \"S17\",\n",
      "#          \"S18\"]\n",
      "# names = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\", \"S6\", \"S7\", \"S8\", \"S9\", \"S10\", \"S11\", \"S12\", \"S13\", \"S14\", \"S15\", \"S16\", \"S17\", \"S18\"]\n",
      "names = [\"S2\"]\n",
      "\n",
      "# 所用的数据目录路径\n",
      "# data_document_path = \"../dataset\"\n",
      "data_document_path = \"../dataset_16\"\n",
      "# data_document_path = \"../dataset_csp\"\n",
      "# data_document_path = \"../dataset_fbcsp\"\n",
      "# data_document_path = \"../data_all_split\"\n",
      "# data_document_path = \"/document/data/eeg/dataset\"\n",
      "# data_document_path = \"/document/data/eeg/dataset_csp\"\n",
      "# data_document_path = \"/document/data/eeg/dataset_fbcsp\"\n",
      "# data_document_path = \"/document/data/eeg/dataset_fbcsp10\"\n",
      "# data_document_path = \"/document/data/eeg/data_all_split\"\n",
      "\n",
      "# 常用模型参数，分别是 重复率、窗长、时延、最大迭代次数、分批训练参数、是否early stop\n",
      "# 其中窗长和时延，因为采样率为70hz，所以70为1秒\n",
      "overlap = 0.5\n",
      "window_length = 140\n",
      "delay = 0\n",
      "batch_size = 1\n",
      "max_epoch = 60\n",
      "min_epoch = 0\n",
      "isEarlyStop = False\n",
      "\n",
      "# 非常用参数，分别是 被试数量、通道数量、trail数量、trail内数据点数量、测试集比例、验证集比例\n",
      "# 一般不需要调整\n",
      "people_number = 18\n",
      "eeg_channel = 16\n",
      "audio_channel = 1\n",
      "channel_number = eeg_channel + audio_channel * 2\n",
      "trail_number = 20\n",
      "cell_number = 3500\n",
      "test_percent = 0.1\n",
      "vali_percent = 0.1\n",
      "\n",
      "conv_eeg_channel = 16\n",
      "conv_audio_channel = 16\n",
      "conv_time_size = 9\n",
      "conv_output_channel = 10\n",
      "fc_number = 30\n",
      "\n",
      "conv_eeg_audio_number = 4\n",
      "output_fc_number = conv_eeg_audio_number * conv_output_channel\n",
      "\n",
      "# 模型选择\n",
      "# True为CNN：D+S或CNN：FM+S模型，False为CNN：S模型\n",
      "isDS = True\n",
      "# isFM为0是男女信息，为1是方向信息\n",
      "isFM = 0\n",
      "# 频带数量\n",
      "bands_number = 1\n",
      "useFB = False\n",
      "\n",
      "# 数据划分选择\n",
      "# 测试集划分是否跨trail\n",
      "isBeyoudTrail = False\n",
      "# 是否使用100%的数据作为训练集，isBeyoudTrail=False、isALLTrain=True、need_pretrain = True、need_train = False说明跨被试\n",
      "isALLTrain = False\n",
      "\n",
      "# 预训练选择\n",
      "# 只有train就是单独训练、只有pretrain是跨被试、两者都有是预训练\n",
      "# 跨被试还需要上方的 isALLTrain 为 True\n",
      "need_pretrain = False\n",
      "need_train = True\n",
      "\n",
      "\n",
      "# 整体模型\n",
      "\n",
      "\n",
      "class CNN(nn.Module):\n",
      "    def __init__(self, embed_dim=30,\n",
      "                 num_heads=5,\n",
      "                 layers=5,\n",
      "                 attn_dropout=0.1,\n",
      "                 relu_dropout=0.1,\n",
      "                 res_dropout=0.1,\n",
      "                 embed_dropout=0.25,\n",
      "                 attn_mask=True):\n",
      "        super(CNN, self).__init__()\n",
      "\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        self.layers = layers\n",
      "        self.attn_dropout = attn_dropout\n",
      "        self.relu_dropout = relu_dropout\n",
      "        self.res_dropout = res_dropout\n",
      "        self.embed_dropout = embed_dropout\n",
      "        self.attn_mask = attn_mask\n",
      "\n",
      "        self.conv1 = nn.ModuleList([nn.Sequential(\n",
      "            nn.Conv1d(conv_audio_channel, conv_output_channel, 8),\n",
      "            nn.ReLU(),\n",
      "            nn.AdaptiveMaxPool1d(1),\n",
      "        ) for i in range(conv_eeg_audio_number)])\n",
      "\n",
      "        self.conv2 = nn.ModuleList([nn.Sequential(\n",
      "            nn.Conv1d(conv_audio_channel, conv_output_channel, 9),\n",
      "            nn.ReLU(),\n",
      "            nn.AdaptiveMaxPool1d(1),\n",
      "        ) for i in range(conv_eeg_audio_number)])\n",
      "\n",
      "        self.conv3 = nn.ModuleList([nn.Sequential(\n",
      "            nn.Conv1d(conv_audio_channel, conv_output_channel, 10),\n",
      "            nn.ReLU(),\n",
      "            nn.AdaptiveMaxPool1d(1),\n",
      "        ) for i in range(conv_eeg_audio_number)])\n",
      "\n",
      "        self.fc = nn.ModuleList([nn.Sequential(\n",
      "            nn.Linear(fc_number, fc_number),\n",
      "            nn.ReLU(),\n",
      "            nn.Dropout(p=0.1),\n",
      "            nn.Linear(fc_number, conv_output_channel)\n",
      "        ) for i in range(conv_eeg_audio_number)])\n",
      "\n",
      "        self.output_fc = nn.Sequential(\n",
      "            nn.Linear(output_fc_number, output_fc_number), nn.Sigmoid(),\n",
      "            nn.Linear(output_fc_number, 2), nn.Sigmoid()\n",
      "        )\n",
      "\n",
      "        self.proj_images = nn.Conv1d(eeg_channel, conv_eeg_channel, 1, padding=0, bias=False)\n",
      "        self.proj_images2 = nn.Conv1d(eeg_channel, conv_eeg_channel, 1, padding=0, bias=False)\n",
      "        self.proj_audio = nn.Conv1d(audio_channel, conv_audio_channel, 1, bias=False)\n",
      "        self.proj_audio2 = nn.Conv1d(audio_channel, conv_audio_channel, 1, bias=False)\n",
      "\n",
      "        self.trans_a2e = self.get_network(self_type='a2e')\n",
      "        self.trans_a2e2 = self.get_network(self_type='a2e')\n",
      "        self.trans_e2a = self.get_network(self_type='e2a')\n",
      "        self.trans_e2a2 = self.get_network(self_type='e2a')\n",
      "\n",
      "    def get_network(self, self_type='a2e', layers=-1):\n",
      "        if self_type in ['a2e']:\n",
      "            embed_dim = 16\n",
      "            num_heads = 1\n",
      "            isAE = True\n",
      "        elif self_type in ['e2a']:\n",
      "            embed_dim = 16\n",
      "            num_heads = 1\n",
      "            isAE = False\n",
      "        else:\n",
      "            raise ValueError(\"Unknown network type\")\n",
      "\n",
      "        return TransformerEncoder(embed_dim=embed_dim,\n",
      "                                  num_heads=num_heads,\n",
      "                                  layers=max(self.layers, layers),\n",
      "                                  attn_dropout=self.attn_dropout,\n",
      "                                  relu_dropout=self.relu_dropout,\n",
      "                                  res_dropout=self.res_dropout,\n",
      "                                  embed_dropout=self.embed_dropout,\n",
      "                                  attn_mask=self.attn_mask,\n",
      "                                  isAE=isAE)\n",
      "\n",
      "    def cross_modal(self, x_audio, x_eeg, is_images=False, conv1D=False):\n",
      "        '''\n",
      "        audio and eeg should have dimension [batch_size, seq_len, n_features]\n",
      "        if x_eeg is eegImages, the dimension should be [batch_size, seq_len, channel, height, width], default the images channel=1\n",
      "        x_audio and x_eeg are both numpy array.\n",
      "        '''\n",
      "\n",
      "        x_audio = x_audio.transpose(1, 2)\n",
      "        x_eeg = x_eeg.transpose(1, 2)\n",
      "\n",
      "        #     proj_x_audio = self.proj_audio(x_audio)\n",
      "        #     proj_x_eeg = self.proj_images(x_eeg)\n",
      "\n",
      "        # proj_x_audio = proj_x_audio.permute(2, 0, 1)\n",
      "        # proj_x_eeg = proj_x_eeg.permute(2, 0, 1)\n",
      "        x_audio = x_audio.permute(2, 0, 1)\n",
      "        x_eeg = x_eeg.permute(2, 0, 1)\n",
      "\n",
      "        audio_trans_eeg, ae, ae_fc = self.trans_a2e(x_audio, x_eeg, x_eeg, True)\n",
      "        audio_trans_eeg = audio_trans_eeg.permute(1, 0, 2)\n",
      "        eeg_trans_audio, ea, ea_fc = self.trans_e2a(x_eeg, x_audio, x_audio, True)\n",
      "        eeg_trans_audio = eeg_trans_audio.permute(1, 0, 2)\n",
      "\n",
      "        return eeg_trans_audio, audio_trans_eeg, ae, ae_fc, ea, ea_fc\n",
      "\n",
      "    def cross_modal2(self, x_audio, x_eeg, is_images=False, conv1D=False):\n",
      "        '''\n",
      "        audio and eeg should have dimension [batch_size, seq_len, n_features]\n",
      "        if x_eeg is eegImages, the dimension should be [batch_size, seq_len, channel, height, width], default the images channel=1\n",
      "        x_audio and x_eeg are both numpy array.\n",
      "        '''\n",
      "\n",
      "        x_audio = x_audio.transpose(1, 2)\n",
      "        x_eeg = x_eeg.transpose(1, 2)\n",
      "\n",
      "        #     proj_x_audio = self.proj_audio2(x_audio)\n",
      "        #     proj_x_eeg = self.proj_images2(x_eeg)\n",
      "\n",
      "        # proj_x_audio = proj_x_audio.permute(2, 0, 1)\n",
      "        # proj_x_eeg = proj_x_eeg.permute(2, 0, 1)\n",
      "        x_audio = x_audio.permute(2, 0, 1)\n",
      "        x_eeg = x_eeg.permute(2, 0, 1)\n",
      "\n",
      "        audio_trans_eeg, ae, ae_fc = self.trans_a2e(x_audio, x_eeg, x_eeg, True)\n",
      "        audio_trans_eeg = audio_trans_eeg.permute(1, 0, 2)\n",
      "        eeg_trans_audio, ea, ea_fc = self.trans_e2a2(x_eeg, x_audio, x_audio, True)\n",
      "        eeg_trans_audio = eeg_trans_audio.permute(1, 0, 2)\n",
      "\n",
      "        return eeg_trans_audio, audio_trans_eeg, ae, ae_fc, ea, ea_fc\n",
      "\n",
      "    def forward(self, x):\n",
      "\n",
      "        wavA = x[0, 0, 0:1, :]\n",
      "        wavA = torch.t(wavA).unsqueeze(0)\n",
      "        eeg = x[0, 0, 1:17, :]\n",
      "        eeg = torch.t(eeg).unsqueeze(0)\n",
      "        wavB = x[0, 0, 17:18, :]\n",
      "        wavB = torch.t(wavB).unsqueeze(0)\n",
      "\n",
      "        # print(x.shape)\n",
      "        # print(wavA.shape)\n",
      "        # print(eeg.shape)\n",
      "        # print(wavB.shape)\n",
      "\n",
      "        wavA = torch.zeros(wavA.shape[0], wavA.shape[1], conv_audio_channel).to(device) + wavA\n",
      "        wavB = torch.zeros(wavA.shape[0], wavB.shape[1], conv_audio_channel).to(device) + wavB\n",
      "\n",
      "        wavA_before = wavA.clone()\n",
      "\n",
      "        eegA, wavA, aeA, ae_fcA, eaA, ea_fcA = self.cross_modal(wavA, eeg, is_images=False, conv1D=True)\n",
      "        eegB, wavB, aeB, ae_fcB, eaB, ea_fcB = self.cross_modal2(wavB, eeg, is_images=False, conv1D=True)\n",
      "\n",
      "        wavA_after = wavA.clone()\n",
      "\n",
      "        # print(\"wavAB\")\n",
      "        # print(wavA.shape)\n",
      "        # print(wavB.shape)\n",
      "\n",
      "        wavA = wavA.transpose(1, 2)\n",
      "        eegA = eegA.transpose(1, 2)\n",
      "        eegB = eegB.transpose(1, 2)\n",
      "        wavB = wavB.transpose(1, 2)\n",
      "\n",
      "        # textCNN\n",
      "        data = [wavA, eegA, eegB, wavB]\n",
      "        for i in range(conv_eeg_audio_number):\n",
      "            # print(wavA.shape)\n",
      "            temp1 = self.conv1[i](data[i]).view(-1, conv_output_channel)\n",
      "            # print(wavA1.shape)\n",
      "            temp2 = self.conv2[i](data[i]).view(-1, conv_output_channel)\n",
      "            # print(wavA2.shape)\n",
      "            temp3 = self.conv3[i](data[i]).view(-1, conv_output_channel)\n",
      "            # print(wavA3.shape)\n",
      "            temp = torch.cat([temp1, temp2, temp3], dim=1)\n",
      "            data[i] = self.fc[i](temp)\n",
      "\n",
      "        x = torch.cat([data[i] for i in range(conv_eeg_audio_number)], dim=1)\n",
      "        output = self.output_fc(x)\n",
      "        return output, aeA, [wavA_before, wavA_after], ae_fcA, eaA, aeB, eaB\n",
      "\n",
      "\n",
      "# 模型权重初始化\n",
      "def weights_init_uniform(m):\n",
      "    classname = m.__class__.__name__\n",
      "    if classname.find('Linear') != -1:\n",
      "        m.weight.data.uniform_(-1.0, 1.0)\n",
      "        m.bias.data.fill_(0)\n",
      "\n",
      "\n",
      "# 模型参数和初始化\n",
      "myNet = CNN()\n",
      "myNet.apply(weights_init_uniform)\n",
      "clip = 0.8\n",
      "optimzer = torch.optim.Adam(myNet.parameters(), lr=0.5)\n",
      "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
      "    optimzer, mode='min', factor=0.5, patience=5, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=5,\n",
      "    min_lr=0.5, eps=0.001)\n",
      "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimzer, T_max = 10, eta_min=0, last_epoch=-1)\n",
      "loss_func = nn.CrossEntropyLoss()\n",
      "\n",
      "# 启用gpu\n",
      "gpu_random = random.randint(5, 7)\n",
      "device = torch.device('cuda:' + str(0))\n",
      "# device = torch.device('cpu')\n",
      "myNet = myNet.to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_func = loss_func.to(device)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "import logging\n",
    "from importlib import reload\n",
    "import re\n",
    "\n",
    "label = \"\"\n",
    "names = []\n",
    "logger = 0\n",
    "result_document = \"./result/\"\n",
    "all_names = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\", \"S6\", \"S7\", \"S8\", \"S9\", \"S10\", \"S11\", \"S12\", \"S13\", \"S14\", \"S15\", \"S16\", \"S17\", \"S18\"]\n",
    "\n",
    "\n",
    "def run_split():\n",
    "    # for i in range(len(all_names)):\n",
    "    for i in range(len(names)):\n",
    "        # os.system(\"nohup /home/lipeiwen/anaconda3/bin/python -u CNN_split.py \" + all_names[i] + \" > \" +\n",
    "        #           result_document + label + \"/CNN_split_\" + all_names[i] + \"_\" + label + \".log 2>&1 &\")\n",
    "        os.system(\"nohup ~/anaconda3/bin/python -u CNN_split.py \" + names[i] + \" > \" +\n",
    "                  result_document + label + \"/CNN_split_\" + names[i] + \"_\" + label + \".log 2>&1 &\")\n",
    "\n",
    "\n",
    "def run_train():\n",
    "    for i in range(len(names)):\n",
    "        os.system(\"nohup ~/anaconda3/bin/python -u CNN.py \" + names[i] + \" > \" +\n",
    "                  result_document + label + \"/CNN_\" + names[i] + \"_\" + label + \".log 2>&1 &\")\n",
    "\n",
    "\n",
    "def __get_last_line(filename):\n",
    "    try:\n",
    "        filesize = os.path.getsize(filename)\n",
    "        if filesize == 0:\n",
    "            return None\n",
    "        else:\n",
    "            with open(filename, 'rb') as fp:  # to use seek from end, must use mode 'rb'\n",
    "                offset = -2                # initialize offset\n",
    "                while -offset < filesize:   # offset cannot exceed file size\n",
    "                    # read # offset chars from eof(represent by number '2')\n",
    "                    fp.seek(offset, 2)\n",
    "                    lines = fp.readlines()  # read from fp to eof\n",
    "                    if len(lines) >= 2:     # if contains at least 2 lines\n",
    "                        # then last line is totally included\n",
    "                        return lines[-1]\n",
    "                    else:\n",
    "                        offset *= 2         # enlarge offset\n",
    "                fp.seek(0)\n",
    "                lines = fp.readlines()\n",
    "                return lines[-1]\n",
    "    except FileNotFoundError:\n",
    "        logger.error(filename + ' not found!')\n",
    "        return None\n",
    "\n",
    "\n",
    "def search_all_files_return_by_time_reversed(path, reverse=True):\n",
    "    return sorted(glob.glob(os.path.join(path, '*')), key=lambda x: time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(os.path.getctime(x))), reverse=reverse)\n",
    "\n",
    "\n",
    "def MonitorSplit():\n",
    "    while True:\n",
    "        isFinish = True\n",
    "        for i in range(len(names)):\n",
    "            filename = result_document + label + \"/CNN_split_\" + names[i] + \"_\" + label + \".log\"\n",
    "            str = __get_last_line(filename).decode()\n",
    "            if (\"finish\" not in str):\n",
    "                isFinish = False\n",
    "                break\n",
    "        if isFinish:\n",
    "            break\n",
    "\n",
    "        time.sleep(60)\n",
    "\n",
    "\n",
    "def MonitorTrain():\n",
    "    while True:\n",
    "        isFinish = True\n",
    "        for i in range(len(names)):\n",
    "            filename = result_document + label + \"/CNN_\" + names[i] + \"_\" + label + \".log\"\n",
    "            str = __get_last_line(filename).decode()\n",
    "            if (not str.startswith(\"0.\") and not str.startswith(\"1.\")):\n",
    "                isFinish = False\n",
    "                break\n",
    "\n",
    "        if isFinish:\n",
    "            break\n",
    "\n",
    "        time.sleep(60)\n",
    "\n",
    "\n",
    "def MonitorParameters():\n",
    "    while True:\n",
    "        file_list = search_all_files_return_by_time_reversed(\"./parameters\")\n",
    "        if len(file_list) > 0:\n",
    "            if os.path.exists(\"./parameters.py\"):\n",
    "                os.remove(\"./parameters.py\")\n",
    "            os.rename(file_list[0], \"./parameters.py\")\n",
    "\n",
    "            break\n",
    "        time.sleep(60)\n",
    "    return\n",
    "\n",
    "\n",
    "def init():\n",
    "    import parameters\n",
    "    reload(parameters)\n",
    "    global label\n",
    "    global names\n",
    "    label = parameters.label\n",
    "    names = parameters.names\n",
    "\n",
    "    if not os.path.exists(result_document + label):\n",
    "        os.mkdir(result_document + label)\n",
    "    shutil.copy(\"./parameters.py\", result_document + label)\n",
    "\n",
    "    reload(logging)\n",
    "    global logger\n",
    "    # 第一步，创建一个logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)   \n",
    "\n",
    "    # 第二步，创建一个handler，用于写入日志文件\n",
    "    logfile = result_document + label + \"/logger.txt\"\n",
    "    fh = logging.FileHandler(logfile, mode='w')\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "\n",
    "    # 第三步，再创建一个handler，用于输出到控制台\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.INFO)  \n",
    "\n",
    "    # 第四步，定义handler的输出格式\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s\")\n",
    "    fh.setFormatter(formatter)\n",
    "    ch.setFormatter(formatter)\n",
    "\n",
    "    # 第五步，将logger添加到handler里面\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    logger.info(\"model label: \" + label)\n",
    "\n",
    "\n",
    "def output_result():\n",
    "    output = \"result: \\n\"\n",
    "    for i in range(len(names)):\n",
    "        filename = result_document + label + \"/CNN_\" + names[i] + \"_\" + label + \".log\"\n",
    "        str = __get_last_line(filename).decode()\n",
    "        output = output + str\n",
    "    logger.info(output)\n",
    "\n",
    "def grid_search(filename, pattern, parameter_range):\n",
    "    for i in range(len(parameter_range)):\n",
    "        file = open(\"parameters.py\", \"r\", encoding=(\"utf-8\"))\n",
    "        string = file.read()\n",
    "        string = re.sub(pattern, parameter_range[i], string)\n",
    "        print(string)\n",
    "        file.close()\n",
    "        \n",
    "        file = open(\"parameters.py\", \"w\", encoding=(\"utf-8\"))\n",
    "        file.write(string)\n",
    "    return None\n",
    "\n",
    "def loopMonitor():\n",
    "    while True:\n",
    "        MonitorParameters()\n",
    "        init()\n",
    "\n",
    "        logger.info(\"split start!\")\n",
    "        run_split()\n",
    "        time.sleep(10)\n",
    "        MonitorSplit()\n",
    "        logger.info(\"split finish!\")\n",
    "\n",
    "        logger.info(\"train start!\")\n",
    "        run_train()\n",
    "        time.sleep(60)\n",
    "        MonitorTrain()\n",
    "        logger.info(\"train finish!\")\n",
    "\n",
    "        output_result()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pattern = \"lr=(-?\\d+)(\\.\\d+)?\"\n",
    "    parameter_range = [\"lr=0.1\", \"lr=0.5\"]\n",
    "    grid_search(\"filename\", pattern, parameter_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
